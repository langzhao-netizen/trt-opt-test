{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to use this notebook\n",
        "\n",
        "**Purpose:** Run inference with a causal LM (e.g. Llama, Mistral) on the toxicity dataset `rungalileo/automated-ft-luna-toxicity`, get per-example probabilities for \"true\"/\"false\" (binary), and compute metrics (AUROC, F1, confusion matrix).\n",
        "\n",
        "**Before running:**\n",
        "\n",
        "1. **Set config (next cell):** `checkpoint_path` = path or HF id of your model (can be base or fine-tuned); `model_name` = tokenizer path (often same as base model); `label_key` = column name of the label in the dataset (e.g. `\"label\"`); `max_seq_length`, `device`.\n",
        "2. **Optional:** Clone `llm-finetuning` in this repo root for `PromptTemplate`. If you don’t, the notebook will use a built-in minimal template (same prompt text).\n",
        "\n",
        "**Dataset:** Default is `rungalileo/automated-ft-luna-toxicity`. It must have a column for the message (template uses `{text}`) and a label column (set `label_key`).\n",
        "\n",
        "**Order:** Run cells top to bottom. The last cells build the test set with the prompt template, run `evaluate()`, then `run_metrics()`."
      ],
      "id": "fe48cda8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------- Config (set these before running the rest) ----------\n",
        "import torch\n",
        "checkpoint_path = \"meta-llama/Llama-3.1-8B-Instruct\"   # or path like \"shared/models/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"        # tokenizer; usually same as base model\n",
        "label_key = \"label\"                                     # column in dataset with 0/1 or true/false\n",
        "max_seq_length = 2048\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "52b2914a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# !git clone https://{github_username}:{github_access_token}@github.com/rungalileo/llm-finetuning"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9fdaa482"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "# PromptTemplate: use llm-finetuning if present, else minimal inline version\n",
        "try:\n",
        "    sys.path.insert(0, 'llm-finetuning')\n",
        "    from llm_finetune.templates.inference_template import PromptTemplate\n",
        "except Exception:\n",
        "    class PromptTemplate:\n",
        "        def __init__(self, name: str, template: str):\n",
        "            self.name, self.template = name, template\n",
        "        def format(self, example: dict) -> str:\n",
        "            return self.template.format(**example)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1a725a56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint_path,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "adfb7d78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# mistral works on 'left' padding instead of 'right', not sure if changing the padding location changes anything\n",
        "tokenizer.padding_side = 'right'\n",
        "tokenizer.truncation_side = 'left'  # keeps the end-tail of the prompt and not the beginning when truncating\n",
        "tokenizer.pad_token = tokenizer.eos_token # keeping this gives an error"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "81d4bce7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds = load_dataset('rungalileo/automated-ft-luna-toxicity')\n",
        "\n",
        "ds_train = ds['train']\n",
        "ds_test  = ds['test']"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a40daed9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mode=\"binary\""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2f363e08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "TEMPLATE = PromptTemplate(\n",
        "    name=\"toxicity\",\n",
        "    template=\"\"\"\\\n",
        "The user will provide you with a text and you need to determine if it is toxic or not. Toxicity refers to language that is harmful or inappropriate, typically evaluated based on the following aspects:\n",
        "Hate Speech: Statements that demean, dehumanize, or attack individuals or groups based on identity factors like race, gender, or religion.\n",
        "Offensive Content: Vulgar, abusive, or overly profane language used to provoke or insult.\n",
        "Sexual Content: Explicit or inappropriate sexual statements that may be offensive or unsuitable in context.\n",
        "Violence or Harm: Advocacy or description of physical harm, abuse, or violent actions.\n",
        "Illegal or Unethical Guidance: Instructions or encouragement for illegal or unethical actions.\n",
        "Manipulation or Exploitation: Language intended to deceive, exploit, or manipulate individuals for harmful purposes.\n",
        "\n",
        "Message:\n",
        "\n",
        "```\n",
        "{text}\n",
        "```\n",
        "\n",
        "Respond with a single word. Respond with \"true\" (no quotes) if the text is toxic, and \"false\" (no quotes) if it is not.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "template = TEMPLATE"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ca892b72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_template(example: dict, tokenizer: AutoTokenizer, template: PromptTemplate,):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": template.format(example)\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    example[\"model_input_text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return example"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e938000f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds_test = ds_test.map(\n",
        "    apply_template,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer, \"template\": template},\n",
        "    desc=\"Applying chat template\"\n",
        ")\n",
        "ds_test"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7232ccf9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probablity Postprocessor factory"
      ],
      "id": "bb7073aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Set, Dict, List\n",
        "from scipy.special import softmax\n",
        "\n",
        "class BooleanProbabilityPostprocessor:\n",
        "    def __init__(self, tokenizer: AutoTokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.vocab_ixs_counted_as_true: Set[int] = set()\n",
        "        self.vocab_ixs_counted_as_false: Set[int] = set()\n",
        "        self.vocab_string_to_id: Dict[str, int] = {}\n",
        "\n",
        "        self._setup()\n",
        "\n",
        "    def _setup(self) -> None:\n",
        "        vocab_ixs = list(range(self.tokenizer.vocab_size))\n",
        "        hf_tokens = self.tokenizer.convert_ids_to_tokens(vocab_ixs)\n",
        "        # cleans up stuff like 'Ġ' for ' ' in HF tokens\n",
        "        vocab_strings = [\n",
        "            self.tokenizer.convert_tokens_to_string([tok]) for tok in hf_tokens\n",
        "        ]\n",
        "\n",
        "        for ix, token in zip(vocab_ixs, vocab_strings):\n",
        "            if token.lower().strip() == \"true\":\n",
        "                self.vocab_ixs_counted_as_true.add(ix)\n",
        "                self.vocab_string_to_id[self.tokenizer.convert_ids_to_tokens([ix])[0]] = ix\n",
        "            elif token.lower().strip() == \"false\":\n",
        "                self.vocab_string_to_id[self.tokenizer.convert_ids_to_tokens([ix])[0]] = ix\n",
        "                self.vocab_ixs_counted_as_false.add(ix)\n",
        "\n",
        "    def postprocess_logits(self, logits: List[float]) -> float:\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        prob_true, prob_false = 0.0, 0.0\n",
        "\n",
        "        for ix in self.vocab_ixs_counted_as_true:\n",
        "            prob_true += probs[ix]\n",
        "        for ix in self.vocab_ixs_counted_as_false:\n",
        "            prob_false += probs[ix]\n",
        "\n",
        "        try:\n",
        "            prob = prob_true / (prob_true + prob_false)\n",
        "        except ZeroDivisionError:\n",
        "            prob = 0.0\n",
        "\n",
        "        return prob\n",
        "    def get_label_token_id(self, label: bool) -> int:\n",
        "        return self.vocab_string_to_id[\"true\"] if label else self.vocab_string_to_id[\"false\"]\n",
        "    def response_formatter(self, label: int)->bool:\n",
        "        return bool(label)\n",
        "\n",
        "class MultiClassProbabilityPostprocessor:\n",
        "    \"\"\"Postprocessor for extracting multi-class probabilities from model logits.\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: AutoTokenizer, class_names: List[str]):\n",
        "        \"\"\"Initialize the postprocessor.\n",
        "\n",
        "        Args:\n",
        "            tokenizer: Tokenizer used by the model\n",
        "            class_names: List of class names to extract probabilities for\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.class_names = class_names\n",
        "        self.num_classes = len(self.class_names)\n",
        "        self.class_name_to_ix: Dict[str, int] = {\n",
        "            class_name: ix for ix, class_name in enumerate(self.class_names)\n",
        "        }\n",
        "        self.class_ix_to_name: Dict[int, str] = {\n",
        "            ix: class_name for class_name, ix in self.class_name_to_ix.items()\n",
        "        }\n",
        "        self.class_index_string_to_vocab_ixs: Dict[str, Set] = {\n",
        "            str(i): set() for i in range(len(self.class_names))\n",
        "        }\n",
        "        self.class_index_string_to_single_vocab_ix: Dict[str, int] = {\n",
        "            str(i): None for i in range(len(self.class_names))\n",
        "        }\n",
        "        self._setup()\n",
        "\n",
        "    def _setup(self) -> None:\n",
        "        \"\"\"Set up vocabulary indices for class name tokens.\"\"\"\n",
        "        vocab_ixs = list(range(self.tokenizer.vocab_size))\n",
        "        hf_tokens = self.tokenizer.convert_ids_to_tokens(vocab_ixs)\n",
        "        # cleans up stuff like 'Ġ' for ' ' in HF tokens\n",
        "        vocab_strings = [\n",
        "            self.tokenizer.convert_tokens_to_string([tok]) for tok in hf_tokens\n",
        "        ]\n",
        "\n",
        "        for ix, token in zip(vocab_ixs, vocab_strings):\n",
        "            clean_token = token.lower().strip()\n",
        "            if clean_token in self.class_index_string_to_vocab_ixs:\n",
        "                self.class_index_string_to_vocab_ixs[clean_token].add(ix)\n",
        "                if clean_token == token:\n",
        "                    self.class_index_string_to_single_vocab_ix[clean_token] = ix\n",
        "\n",
        "    def postprocess_logits(self, logits: List[float]) -> List[float]:\n",
        "        \"\"\"Extract multi-class probabilities from logits.\n",
        "\n",
        "        Args:\n",
        "            logits: Model logits with shape (n_vocab,)\n",
        "\n",
        "        Returns:\n",
        "            List of probabilities for each class\n",
        "        \"\"\"\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        class_probs_list = [0.0] * self.num_classes\n",
        "\n",
        "        for i in range(self.num_classes):\n",
        "            # Get the string for that index (e.g., 0 -> \"0\")\n",
        "            class_index_str = str(i)\n",
        "\n",
        "            # Look up all vocab IDs for that string (e.g., \"0\" -> {512, 1923})\n",
        "            for ix in self.class_index_string_to_vocab_ixs[class_index_str]:\n",
        "                class_probs_list[i] += probs[ix]\n",
        "\n",
        "        # Normalize\n",
        "        total_prob = sum(class_probs_list)\n",
        "        if total_prob > 0:\n",
        "            class_probs_list = [\n",
        "                class_prob / total_prob for class_prob in class_probs_list\n",
        "            ]\n",
        "\n",
        "        return class_probs_list\n",
        "\n",
        "    def get_label_token_id(self, label: int) -> int:\n",
        "        return self.class_index_string_to_single_vocab_ix[str(label)]\n",
        "\n",
        "    def response_formatter(self, label: str) -> int:\n",
        "        if label not in self.class_name_to_ix:\n",
        "            raise ValueError(f\"Label '{label}' not found in class_name_to_ix: {list(self.class_name_to_ix.keys())}\")\n",
        "        return self.class_name_to_ix[label]\n",
        "\n",
        "class PostprocessorFactory:\n",
        "    @staticmethod\n",
        "    def get(mode: str, tokenizer: AutoTokenizer, class_names: Optional[Sequence[str]] = None):\n",
        "        mode = mode.strip().lower()\n",
        "        if mode == \"binary\":\n",
        "            return BooleanProbabilityPostprocessor(tokenizer)\n",
        "        elif mode == \"multiclass\":\n",
        "            if class_names is None:\n",
        "                raise ValueError(\"class_names must be provided for multiclass mode.\")\n",
        "            return MultiClassProbabilityPostprocessor(tokenizer, class_names)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode '{mode}'. Use 'binary' or 'multiclass'.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cba1b91b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ],
      "id": "2a654b3d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class SingleTokenClassificationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Stores per-example: input_ids (list), attention_mask (list), label_token_id (int), example_level_label (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, hf_dataset, tokenizer, max_seq_length, mode=\"binary\", class_names=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.processor = PostprocessorFactory.get(mode, tokenizer, class_names)\n",
        "        self.samples = []\n",
        "\n",
        "        for example in tqdm(hf_dataset, desc=\"Preparing dataset\"):\n",
        "            prompt = example[\"model_input_text\"]\n",
        "            label = self.processor.response_formatter(example[label_key])\n",
        "\n",
        "            # Keep last tokens: we want the prompt tail that predicts the label token next.\n",
        "            enc = tokenizer(\n",
        "                prompt,\n",
        "                truncation=True,\n",
        "                max_length=max_seq_length,\n",
        "                add_special_tokens=False    # TODO: not sure of this line\n",
        "            )\n",
        "\n",
        "            # label_token_id is the token id used during training target (single-token guarantee)\n",
        "            label_token_id = self.processor.get_label_token_id(label)\n",
        "\n",
        "            sample = {\n",
        "                \"input_ids\": enc[\"input_ids\"],              # keep as list (collator will pad)\n",
        "                \"attention_mask\": enc[\"attention_mask\"],    # list of 0/1\n",
        "                \"label_token_id\": label_token_id,\n",
        "                \"example_level_label\": label\n",
        "            }\n",
        "            self.samples.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e298a682"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collator"
      ],
      "id": "024ebaad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create once in notebook (use same tokenizer variable you already have)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Accepts a list of dicts from SingleTokenClassificationDataset.\n",
        "    Uses HF DataCollatorWithPadding to pad input_ids/attention_mask, then stacks label_token_ids.\n",
        "    \"\"\"\n",
        "    # Extract only the keys HF collator expects and let it pad\n",
        "    model_inputs = [\n",
        "        {k: v for k, v in item.items() if k in ['input_ids', 'attention_mask']}   # TODO: token type ids not sure\n",
        "        for item in batch\n",
        "    ]\n",
        "    padded = data_collator(model_inputs)  # returns dict of tensors: input_ids, attention_mask, ...\n",
        "\n",
        "    # Add label_token_ids as a tensor (B,)\n",
        "    padded[\"label_token_ids\"] = torch.tensor([item[\"label_token_id\"] for item in batch], dtype=torch.long)\n",
        "\n",
        "    padded[\"example_level_label\"] = torch.tensor([item[\"example_level_label\"] for item in batch], dtype=torch.long)\n",
        "\n",
        "    return padded\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e0d4c161"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ],
      "id": "2e48807b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_metrics(true_labels, predictions, threshold=0.5, figsize=(18, 12)):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a binary classifier in a single function.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    true_labels : array-like\n",
        "        Ground truth binary labels (0, 1)\n",
        "    predictions : array-like\n",
        "        Predicted probabilities for the positive class\n",
        "    threshold : float, default=0.5\n",
        "        Decision threshold for binary classification\n",
        "    figsize : tuple, default=(18, 12)\n",
        "        Size of the figure for plots\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None - displays results and visualizations\n",
        "    \"\"\"\n",
        "    # Convert inputs to numpy arrays for consistency\n",
        "    true_labels = np.array(true_labels)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Create binary predictions using threshold\n",
        "    binary_preds = (predictions >= threshold).astype(int)\n",
        "\n",
        "    # Calculate ROC curve and AUROC\n",
        "    fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
        "    auroc = roc_auc_score(true_labels, predictions)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(true_labels, binary_preds),\n",
        "        'f1': f1_score(true_labels, binary_preds),\n",
        "        'precision': precision_score(true_labels, binary_preds),\n",
        "        'recall': recall_score(true_labels, binary_preds)\n",
        "    }\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, binary_preds)\n",
        "\n",
        "    # Create a DataFrame for easier visualization\n",
        "    df = pd.DataFrame({'true_labels': true_labels, 'predictions': predictions})\n",
        "\n",
        "    # Create the visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    axes[0,0].plot(fpr, tpr, 'b-', linewidth=2)\n",
        "    axes[0,0].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "    axes[0,0].set_xlim([0, 1])\n",
        "    axes[0,0].set_ylim([0, 1])\n",
        "    axes[0,0].set_aspect('equal')\n",
        "    axes[0,0].set_xlabel('False Positive Rate')\n",
        "    axes[0,0].set_ylabel('True Positive Rate')\n",
        "    axes[0,0].set_title(f'ROC Curve (AUROC = {auroc:.3f})')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot histograms of predictions by class\n",
        "    for i, label in enumerate([0, 1]):\n",
        "        mask = (true_labels == label)\n",
        "        axes[0,1].hist(predictions[mask], bins=np.linspace(0, 1, 21),\n",
        "                 alpha=0.6, label=f'Class {label}', density=True)\n",
        "\n",
        "    axes[0,1].axvline(threshold, color='red', linestyle='--', linewidth=1)\n",
        "    axes[0,1].set_xlabel('Prediction Probability')\n",
        "    axes[0,1].set_ylabel('Density')\n",
        "    axes[0,1].set_title(f'Prediction Distributions (threshold = {threshold:.2f})')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot Precision-Recall curve\n",
        "    precision, recall, _ = precision_recall_curve(true_labels, predictions)\n",
        "    axes[1,0].plot(recall, precision, color='blue', lw=2)\n",
        "    axes[1,0].set_xlabel('Recall')\n",
        "    axes[1,0].set_ylabel('Precision')\n",
        "    axes[1,0].set_title('Precision-Recall Curve')\n",
        "    axes[1,0].grid()\n",
        "\n",
        "    # Create a custom visualization of the confusion matrix\n",
        "    axes[1,1].imshow(cm, cmap='Blues', interpolation='nearest')\n",
        "    axes[1,1].set_title('Confusion Matrix')\n",
        "    axes[1,1].set_xlabel('Predicted Label')\n",
        "    axes[1,1].set_ylabel('True Label')\n",
        "\n",
        "    # Add text annotations to the confusion matrix\n",
        "    thresh = cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            axes[1,1].text(j, i, format(cm[i, j], 'd'),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print metrics in a well-formatted way\n",
        "    print(\"Classification Metrics:\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Threshold: {threshold:.3f}\")\n",
        "    print(f\"AUROC:     {auroc:.3f}\")\n",
        "    print(\"-\" * 40)\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name.capitalize():<10} {value:.3f}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "\n",
        "def run_multiclass_metrics(true_labels, predictions, class_names=None, figsize=(20, 15)):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a multi-class classifier in a single function.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    true_labels : array-like\n",
        "        Ground truth labels (integers from 0 to n_classes-1)\n",
        "    predictions : array-like\n",
        "        Either predicted probabilities (2D array: n_samples x n_classes)\n",
        "        or predicted class labels (1D array)\n",
        "    class_names : list, optional\n",
        "        Names of the classes for better visualization\n",
        "    figsize : tuple, default=(20, 15)\n",
        "        Size of the figure for plots\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None - displays results and visualizations\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    from sklearn.metrics import (\n",
        "        accuracy_score, f1_score, precision_score, recall_score,\n",
        "        confusion_matrix, classification_report, roc_curve, auc,\n",
        "        precision_recall_curve, average_precision_score\n",
        "    )\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    from itertools import cycle\n",
        "\n",
        "    # Convert inputs to numpy arrays\n",
        "    true_labels = np.array(true_labels)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Determine if predictions are probabilities or class labels\n",
        "    if predictions.ndim == 2:\n",
        "        # Probabilities provided\n",
        "        pred_probs = predictions\n",
        "        pred_labels = np.argmax(predictions, axis=1)\n",
        "        n_classes = predictions.shape[1]\n",
        "    else:\n",
        "        # Class labels provided\n",
        "        pred_labels = predictions\n",
        "        n_classes = len(np.unique(np.concatenate([true_labels, pred_labels])))\n",
        "        pred_probs = None\n",
        "\n",
        "    # Set up class names\n",
        "    if class_names is None:\n",
        "        class_names = [f'Class {i}' for i in range(n_classes)]\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    f1_macro = f1_score(true_labels, pred_labels, average='macro')\n",
        "    f1_micro = f1_score(true_labels, pred_labels, average='micro')\n",
        "    f1_weighted = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    precision_macro = precision_score(true_labels, pred_labels, average='macro')\n",
        "    precision_micro = precision_score(true_labels, pred_labels, average='micro')\n",
        "    precision_weighted = precision_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    recall_macro = recall_score(true_labels, pred_labels, average='macro')\n",
        "    recall_micro = recall_score(true_labels, pred_labels, average='micro')\n",
        "    recall_weighted = recall_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "    # Create visualization\n",
        "    if pred_probs is not None:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
        "    else:\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = axes[0, 0] if pred_probs is not None else axes[0, 0]\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
        "    ax1.set_title('Confusion Matrix')\n",
        "    ax1.set_xlabel('Predicted Label')\n",
        "    ax1.set_ylabel('True Label')\n",
        "\n",
        "    # Plot 2: Per-class metrics\n",
        "    ax2 = axes[0, 1] if pred_probs is not None else axes[0, 1]\n",
        "    per_class_f1 = f1_score(true_labels, pred_labels, average=None)\n",
        "    per_class_precision = precision_score(true_labels, pred_labels, average=None)\n",
        "    per_class_recall = recall_score(true_labels, pred_labels, average=None)\n",
        "\n",
        "    x = np.arange(len(class_names))\n",
        "    width = 0.25\n",
        "\n",
        "    ax2.bar(x - width, per_class_precision, width, label='Precision', alpha=0.8)\n",
        "    ax2.bar(x, per_class_recall, width, label='Recall', alpha=0.8)\n",
        "    ax2.bar(x + width, per_class_f1, width, label='F1-Score', alpha=0.8)\n",
        "\n",
        "    ax2.set_xlabel('Classes')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.set_title('Per-Class Metrics')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(class_names, rotation=45)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    if pred_probs is not None:\n",
        "        # Plot 3: ROC Curves (One-vs-Rest)\n",
        "        ax3 = axes[0, 2]\n",
        "\n",
        "        # Binarize the output\n",
        "        y_bin = label_binarize(true_labels, classes=range(n_classes))\n",
        "\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green', 'purple'])\n",
        "\n",
        "        for i, color in zip(range(n_classes), colors):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], pred_probs[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            ax3.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                    label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "        ax3.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        ax3.set_xlim([0.0, 1.0])\n",
        "        ax3.set_ylim([0.0, 1.05])\n",
        "        ax3.set_xlabel('False Positive Rate')\n",
        "        ax3.set_ylabel('True Positive Rate')\n",
        "        ax3.set_title('ROC Curves (One-vs-Rest)')\n",
        "        ax3.legend(loc=\"lower right\")\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Precision-Recall Curves\n",
        "        ax4 = axes[1, 0]\n",
        "\n",
        "        for i, color in zip(range(n_classes), colors):\n",
        "            precision, recall, _ = precision_recall_curve(y_bin[:, i], pred_probs[:, i])\n",
        "            avg_precision = average_precision_score(y_bin[:, i], pred_probs[:, i])\n",
        "            ax4.plot(recall, precision, color=color, lw=2,\n",
        "                    label=f'{class_names[i]} (AP = {avg_precision:.2f})')\n",
        "\n",
        "        ax4.set_xlabel('Recall')\n",
        "        ax4.set_ylabel('Precision')\n",
        "        ax4.set_title('Precision-Recall Curves')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 5: Prediction Distribution\n",
        "        ax5 = axes[1, 1]\n",
        "\n",
        "        for i in range(n_classes):\n",
        "            class_mask = (true_labels == i)\n",
        "            if np.sum(class_mask) > 0:\n",
        "                ax5.hist(pred_probs[class_mask, i], bins=20, alpha=0.6,\n",
        "                        label=f'True {class_names[i]}', density=True)\n",
        "\n",
        "        ax5.set_xlabel('Prediction Probability')\n",
        "        ax5.set_ylabel('Density')\n",
        "        ax5.set_title('Prediction Distributions')\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 6: Class Distribution\n",
        "        ax6 = axes[1, 2]\n",
        "\n",
        "        unique, counts = np.unique(true_labels, return_counts=True)\n",
        "        ax6.bar([class_names[i] for i in unique], counts, alpha=0.7)\n",
        "        ax6.set_xlabel('Classes')\n",
        "        ax6.set_ylabel('Count')\n",
        "        ax6.set_title('Class Distribution')\n",
        "        ax6.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        for i, v in enumerate(counts):\n",
        "            ax6.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
        "\n",
        "    else:\n",
        "        # Without probabilities, show simpler plots\n",
        "\n",
        "        # Plot 3: Class Distribution\n",
        "        ax3 = axes[1, 0]\n",
        "        unique, counts = np.unique(true_labels, return_counts=True)\n",
        "        ax3.bar([class_names[i] for i in unique], counts, alpha=0.7)\n",
        "        ax3.set_xlabel('Classes')\n",
        "        ax3.set_ylabel('Count')\n",
        "        ax3.set_title('True Class Distribution')\n",
        "        ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        for i, v in enumerate(counts):\n",
        "            ax3.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
        "\n",
        "        # Plot 4: Normalized Confusion Matrix\n",
        "        ax4 = axes[1, 1]\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=ax4)\n",
        "        ax4.set_title('Normalized Confusion Matrix')\n",
        "        ax4.set_xlabel('Predicted Label')\n",
        "        ax4.set_ylabel('True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print comprehensive metrics\n",
        "    print(\"Multi-class Classification Metrics\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Accuracy:           {accuracy:.3f}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Macro Averages:\")\n",
        "    print(f\"  Precision:        {precision_macro:.3f}\")\n",
        "    print(f\"  Recall:           {recall_macro:.3f}\")\n",
        "    print(f\"  F1-Score:         {f1_macro:.3f}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Micro Averages:\")\n",
        "    print(f\"  Precision:        {precision_micro:.3f}\")\n",
        "    print(f\"  Recall:           {recall_micro:.3f}\")\n",
        "    print(f\"  F1-Score:         {f1_micro:.3f}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Weighted Averages:\")\n",
        "    print(f\"  Precision:        {precision_weighted:.3f}\")\n",
        "    print(f\"  Recall:           {recall_weighted:.3f}\")\n",
        "    print(f\"  F1-Score:         {f1_weighted:.3f}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Print per-class metrics\n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i in range(n_classes):\n",
        "        print(f\"{class_names[i]:<15} Precision: {per_class_precision[i]:.3f}, \"\n",
        "              f\"Recall: {per_class_recall[i]:.3f}, F1: {per_class_f1[i]:.3f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(classification_report(true_labels, pred_labels, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d44d0d2f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate(model, tokenizer, ds_test, batch_size, mode='binary', class_names=None):\n",
        "    \"\"\"\n",
        "    Returns: (example_true_labels: np.array, example_preds: np.array(prob_true))\n",
        "    Uses custom_collate_fn defined above and BooleanProbabilityPostprocessor for mapping logits -> prob_true.\n",
        "    \"\"\"\n",
        "    eds = SingleTokenClassificationDataset(ds_test, tokenizer, max_seq_length, mode=mode, class_names=class_names)\n",
        "    dataloader = DataLoader(eds, batch_size=batch_size, collate_fn=custom_collate_fn, num_workers=1)\n",
        "    ppp = PostprocessorFactory.get(mode, tokenizer, class_names)\n",
        "\n",
        "    example_true_labels = []\n",
        "    example_preds = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, total=len(eds)//batch_size + (len(eds) % batch_size != 0)):\n",
        "            # collect true labels (example level)\n",
        "            batch_example_level_label = batch.pop('example_level_label')\n",
        "            example_true_labels.extend(batch_example_level_label.tolist())\n",
        "\n",
        "            # move inputs to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
        "            logits = outputs.logits  # (B, S, V)\n",
        "\n",
        "            # compute last unmasked position per row from attention_mask\n",
        "            positions = (batch[\"attention_mask\"].sum(dim=1) - 1).long().to(logits.device)\n",
        "            batch_idx = torch.arange(logits.size(0), device=logits.device)\n",
        "            logits_at_final_position = logits[batch_idx, positions, :]  # (B, V)\n",
        "\n",
        "            # convert to numpy logits and apply your postprocessor (which does softmax internally)\n",
        "            logits_np = logits_at_final_position.cpu().float().numpy()\n",
        "            batch_preds = [ppp.postprocess_logits(row) for row in logits_np]\n",
        "\n",
        "            if np.isnan(batch_preds).any():\n",
        "                raise ValueError(\"NaN predicted probability in evaluation.\")\n",
        "            example_preds.extend(batch_preds)\n",
        "\n",
        "    example_true_labels = np.asarray(example_true_labels)\n",
        "    example_preds = np.asarray(example_preds)\n",
        "    return example_true_labels, example_preds"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9ccca48d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "example_true_labels, example_preds = evaluate(\n",
        "      model,\n",
        "      tokenizer,\n",
        "      ds_test,\n",
        "      batch_size=1,\n",
        "      mode=mode\n",
        "  )\n",
        "run_metrics(example_true_labels, example_preds)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d058fe32"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}